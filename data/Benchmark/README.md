# Benchmark Data and Results

This directory houses the toxic content detection benchmarks (datasets) and the raw results generated by the evaluation scripts.

## Directory Structure

```
data/Benchmark/
├── JigsawBilingualBalanced.csv  # Bilingual/English benchmark
├── ToxiFrenchBalanced.csv       # French benchmark
├── HateDay-french.csv           # Additional French dataset 
├── RTP-LX-french.csv            # Additional French dataset 
└── results/
    ├── metrics_summary.csv      # Aggregated performance metrics
    └── *.csv                    # Raw prediction files (one per experiment)
```

## Datasets

The CSV files at the root serve as the Ground Truth for the evaluation.

| File | Language | Usage |
| :--- | :--- | :--- |
| `ToxiFrenchBalanced.csv` | French | Primary French benchmark (`-tf-fr` results). |
| `JigsawBilingualBalanced.csv` | Bilingual (EN/FR) | Used for English and cross-lingual experiments (`-jigsaw-en`, `-jigsaw-fr` results). |
| `HateDay-french.csv` | French | Supplementary French dataset for extended evaluation. |
| `RTP-LX-french.csv` | French | Supplementary French dataset for extended

## Results

### Raw Prediction Files (`*.csv`)

Each file contains the predictions from a single model/prompt configuration. The naming convention is typically: `[model]-[config]-[benchmark]-[language].csv`.

| Field | Content |
| :--- | :--- |
| `content` | Original input text from the benchmark. |
| `label` | Model's binary prediction (0: non-toxic, 1: toxic). |

### Metrics Summary (`metrics_summary.csv`)

This file is automatically updated and provides aggregated performance metrics for all executed experiments, enabling direct comparison.

| Metric | Class 0 (Non-Toxic) | Class 1 (Toxic) | Overall |
| :--- | :--- | :--- | :--- |
| **Precision** | `Precision_0` | `Precision_1` | - |
| **Recall** | `Recall_0` | `Recall_1` | - |
| **F1 Score** | `F1_0` | `F1_1` | - |
| **Other** | - | - | `Accuracy`, `ROC_AUC` |

-----

## Running Experiments

To run a specific test defined in `config.json` (e.g., to generate a new CSV output and update the summary):

```bash
python benchmark.py --config config.json --experiment "Experiment-Name-from-config-json"
```