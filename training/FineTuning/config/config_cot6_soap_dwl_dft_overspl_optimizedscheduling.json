{
    "model_params": {
        "model_name": "Qwen/Qwen3-4B",
        "local_checkpoint": null,
        "mode": "train",
        "optimizer_type": "SOAP",
        "proxy_address": "http://127.0.0.1:10809"
    },
    "bnb_params": {
        "load_in_4bit": true,
        "bnb_4bit_use_double_quant": true,
        "bnb_4bit_quant_type": "nf4",
        "bnb_4bit_compute_dtype": "bfloat16"
    },
    "dataset_params": {
        "dataset_path": "Naela00/ToxiFrench",
        "text_field": "content",
        "cot_fields": ["CoT_explication", "CoT_tons", "CoT_intentions", "CoT_categorie", "CoT_score", "cot_final_question"],
        "label_field": "literal_conclusion_annotator"
    },
    "training_params": {
        "output_dir": "../../TrainedModels/finetuning/Qwen3-4B-CoT6-SOAP-DFT-OverSpl",
        "num_train_epochs": 3,
        "per_device_train_batch_size": 4,
        "per_device_eval_batch_size": 4,
        "auto_find_batch_size": false,
        "learning_rate": 2e-4,
        "weight_decay": 0.01,
        "warmup_steps": 15,
        "logging_strategy": "steps",
        "logging_steps": 30,
        "save_strategy": "steps",
        "save_steps": 30,
        "save_total_limit": 3,
        "run_name": "Qwen3-4B-CoT6-SOAP-DFT-OverSpl",
        "report_to": "tensorboard",
        "eval_strategy": "steps",
        "eval_steps": 30,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": false,
        "lr_scheduler_type": "cosine",
        "max_length": 1024,
        "fp16": false,
        "bf16": true,
        "torch_compile": false,
        "gradient_checkpointing": true,
        "gradient_accumulation_steps": 32,
        "logging_dir": "../../TrainedModels/finetuning/runs/Qwen3-4B-CoT6-SOAP-DFT-OverSpl",
        "deepspeed": "config/ds_config.json",
        "remove_unused_columns": false,
        "seed": 42
    },
    "training_params_custom": {
        "weight_schedule": [
            {
                "epoch": 0, 
                "alphas": [0.5, 3.0, 1.0, 1.0, 0.5, 0.5, 0.5],
                "comment": "Initial focus on the firt CoT (Explication)"
            },
            {
                "epoch": 1.0, 
                "alphas": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                "comment": "Balance between all tasks"
            },
            {
                "epoch": 2.0, 
                "alphas": [3.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.0],
                "comment": "The final label (index 0) becomes a priority, the CoTs take a back seat"
            },
            {
                "epoch": 2.5, 
                "alphas": [5.0, 0.4, 0.4, 0.4, 0.4, 0.4, 0.0],
                "comment": "Final specialization phase on the conclusion"
            }
        ],
        "DFT": true,
        "dataset": {
            "oversampling": true,
            "seed": 42,
            "train_split_name": "train",
            "eval_split_name": "test"
        }
    },
    "lora_params": {
        "r": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "bias": "none"
    }
}